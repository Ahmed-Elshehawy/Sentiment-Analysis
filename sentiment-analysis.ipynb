{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "7e910277-f7e1-47b1-8991-96a8e3477edb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import mlflow\n",
    "import git\n",
    "import dvc.api\n",
    "import numpy as np\n",
    "from string import punctuation\n",
    "from collections import Counter\n",
    "import pickle\n",
    "from spacy.lang.en import STOP_WORDS\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "import torch.nn as nn\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "56b42130-bfbe-457e-8abc-4abc32a69048",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter your DAGsHub username:  elshehawy\n",
      "Enter your DAGsHub access token:  ········································\n",
      "Enter your DAGsHub project name:  sentiment-analysis\n"
     ]
    }
   ],
   "source": [
    "os.environ['MLFLOW_TRACKING_USERNAME'] = input('Enter your DAGsHub username: ')\n",
    "os.environ['MLFLOW_TRACKING_PASSWORD'] = getpass('Enter your DAGsHub access token: ')\n",
    "os.environ['MLFLOW_TRACKING_PROJECTNAME'] = input('Enter your DAGsHub project name: ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3b86af95-5622-42d5-9628-33ad0471ded3",
   "metadata": {},
   "outputs": [],
   "source": [
    "mlflow.set_tracking_uri('https://dagshub.com/elshehawy/sentiment-analysis.mlflow')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1994242c-95ae-482e-99ad-35c73479653f",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_path = './data/labels.txt'\n",
    "reviews_path = './data/reviews.txt'\n",
    "\n",
    "with open(labels_path, 'r') as f:\n",
    "    labels = f.read()\n",
    "with open(reviews_path, 'r') as f:\n",
    "    reviews = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ef7ca49d-7318-4f6e-995a-5dd96450af4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bromwell high is a cartoon comedy . it ran at the same time as some other programs about school life  such as  teachers  . my   years in the teaching profession lead me to believe that bromwell high  s satire is much closer to reality than is  teachers  . the scramble to survive financially  the insightful students who can see right through their pathetic teachers  pomp  the pettiness of the whole situation  all remind me of the schools i knew and their students . when i saw the episode in which a student repeatedly tried to burn down the school  i immediately recalled . . . . . . . . . at . . . . . . . . . . high . a classic line inspector i  m here to sack one of your teachers . student welcome to bromwell high . i expect that many adults of my age think that bromwell high is far fetched . what a pity that it isn  t   \n",
      "story of a man who has unnatural feelings for a pig . starts out with a opening scene that is a terrific example of absurd comedy . a formal orchestra audience is turned into an insane  violent mob by the crazy chantings of it  s singers . unfortunately it stays absurd the whole time with no general narrative eventually making it just too off putting . even those from the era should be turned off . the cryptic dialogue would make shakespeare seem easy to a third grader . on a technical level it  s better than you might think with some good cinematography by future great vilmos zsigmond . future stars sally kirkland and frederic forrest can be seen briefly .  \n",
      "homelessness  or houselessness as george carlin stated  has been an issue for years but never a plan to help those on the street that were once considered human who did everything from going to school  work  or vote for the matter . most people think of the homeless as just a lost cause while worrying about things such as racism  the war on iraq  pressuring kids to succeed  technology  the elections  inflation  or worrying if they  ll be next to end up on the streets .  br    br   but what if y\n",
      "\n",
      "positive\n",
      "negative\n",
      "positive\n"
     ]
    }
   ],
   "source": [
    "print(reviews[:2000])\n",
    "print()\n",
    "print(labels[:26])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "419695f7-90ce-497d-8fcf-85ff8fae25e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "!\"#$%&'()*+,-./:;<=>?@[\\]^_`{|}~\n"
     ]
    }
   ],
   "source": [
    "from string import punctuation\n",
    "print(punctuation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e73286ec-1de4-452c-ae27-7639e72b6836",
   "metadata": {},
   "outputs": [],
   "source": [
    "import git\n",
    "import dvc.api"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8ec4decf-c0a4-4796-a257-4adc7ee0d4aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO: 'process data' does not exist. Creating a new experiment\n"
     ]
    }
   ],
   "source": [
    "mlflow.set_experiment('process data')\n",
    "with mlflow.start_run(run_name=\"create words\"):\n",
    "    reviews = reviews.lower()\n",
    "    \n",
    "    all_text = ''.join([c for c in reviews if c not in punctuation])\n",
    "    reviews_split = all_text.split('\\n')\n",
    "    all_text = ' '.join(reviews_split)\n",
    "    \n",
    "    with open(reviews_path, 'w') as f:\n",
    "        f.write(all_text)\n",
    "        \n",
    "    mlflow.log_param(\"operation\", 'split and remove punctuation')\n",
    "    mlflow.log_param(\"requested version\", 'v1')\n",
    "    mlflow.log_param('version', 'v2')\n",
    "    mlflow.log_param('number of characters', len(all_text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "bf3dce63-aff5-4d2f-9310-304b030a5dc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9ad9d52f-8a8b-4a34-9465-11dbd4f3a39c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-12-07 20:20:46.443611: W tensorflow/stream_executor/platform/default/dso_loader.cc:60] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2021-12-07 20:20:46.443656: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n"
     ]
    }
   ],
   "source": [
    "from spacy.lang.en import STOP_WORDS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a0e7e5fb-6fd1-4f71-876b-e53ce50c2e6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5b1d7ca9-2838-4484-b7be-6c4f9255e159",
   "metadata": {},
   "outputs": [],
   "source": [
    "with mlflow.start_run(run_name=\"create vocab_to_int file\"):\n",
    "    with open(reviews_path, 'r') as f:\n",
    "        all_text = f.read()\n",
    "    \n",
    "    words = all_text.split()\n",
    "    words = [word for word in words if word not in STOP_WORDS]\n",
    "    counter = Counter(words)\n",
    "    vocab = sorted(counter, key=counter.get, reverse=True)\n",
    "    # Build a dictionary that maps words to integers\n",
    "    vocab_to_int = {word: i for i, word in enumerate(vocab, 1)}    \n",
    "    \n",
    "    file_name = './data/vocab_to_int.sav'\n",
    "    pickle.dump(vocab_to_int, open(file_name, 'wb'))\n",
    "        \n",
    "    mlflow.log_param(\"operation\", 'create vocab to int file')\n",
    "    mlflow.log_param(\"requested version\", 'v2')\n",
    "    mlflow.log_param('version', 'vocab_v1')\n",
    "    mlflow.log_param('number of characters', 'N/A')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "8730ddd0-864d-4972-9f1c-276e0d5f3cb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "with mlflow.start_run(run_name=\"tokenize reviews\"):  \n",
    "    ## use the dict to tokenize each review in reviews_split\n",
    "    ## store the tokenized reviews in reviews_ints\n",
    "    reviews_ints = []\n",
    "    for review in reviews_split:\n",
    "        reviews_ints.append([vocab_to_int[word] for word in review.split() if word not in STOP_WORDS])\n",
    "    \n",
    "    mlflow.log_param(\"operation\", 'tokenize reviews')\n",
    "    mlflow.log_param(\"requested version\", 'N/A')\n",
    "    mlflow.log_param('version', 'rev_tok_v1')\n",
    "    mlflow.log_param('number of characters', len(reviews_ints))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "97c6784b-00e5-4b15-8014-afa99a7c2dbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "with mlflow.start_run(run_name=\"encode labels\"):\n",
    "    \n",
    "    labels_split = labels.split('\\n')\n",
    "    encoded_labels = np.array([1 if label=='positive' else 0 for label in labels_split])\n",
    "        \n",
    "    mlflow.log_param(\"operation\", 'encode labels')\n",
    "    mlflow.log_param(\"requested version\", 'v1')\n",
    "    mlflow.log_param('version', 'v2')\n",
    "    mlflow.log_param('number of characters', len(encoded_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "56f0c37e-6338-45a0-ab03-5e29e9d6ea44",
   "metadata": {},
   "outputs": [],
   "source": [
    "with mlflow.start_run(run_name=\"remove outliers\"):\n",
    "\n",
    "    review_lens = Counter([len(x) for x in reviews_ints])\n",
    "    non_zero_idx = [ii for ii, review in enumerate(reviews_ints) if len(review) !=0]\n",
    "    reviews_ints = [reviews_ints[ii] for ii in non_zero_idx]\n",
    "    encoded_labels = np.array([encoded_labels[ii] for ii in non_zero_idx])\n",
    "        \n",
    "    mlflow.log_param(\"operation\", 'remove outliers')\n",
    "    mlflow.log_param(\"requested version\", 'v2')\n",
    "    mlflow.log_param('version', 'v3')\n",
    "    mlflow.log_param('number of characters', len(encoded_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "6f86e073-e49f-4244-8d4c-ed6c7c3cad91",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad_features(reviews_ints, seq_length):\n",
    "    features = np.zeros((len(reviews_ints), seq_length), dtype=int)\n",
    "    \n",
    "    for i, review in enumerate(reviews_ints):\n",
    "        features[i, -len(review):] = np.array(review)[:seq_length]\n",
    "    \n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "03f7566c-0bd9-4db9-a5d0-b5143ee7e14c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection_selection import train_test_split "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "83715c93-7529-4bce-85f3-e6d87bf5ad1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "with mlflow.start_run(run_name=\"pad features\"):\n",
    "\n",
    "    seq_length = 200 \n",
    "    features = pad_features(reviews_ints, seq_length=seq_length) \n",
    "    split_frac = 0.2\n",
    "    ## split data into training, validation, and test data (features and labels, x and y)\n",
    "    X = features\n",
    "    y = encoded_labels\n",
    "    X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=split_frac, shuffle=True,random_state=42, stratify=y)\n",
    "    X_val, X_test, y_val, y_test = train_test_split(X_val, y_val, test_size=0.5, shuffle=True, random_state=42, stratify=y_val)\n",
    "        \n",
    "    mlflow.log_param(\"operation\", 'pad featues')\n",
    "    mlflow.log_param(\"requested version\", 'v3')\n",
    "    mlflow.log_param('version', 'v4')\n",
    "    mlflow.log_param('number of characters', '0.8, 0.1, 0.1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "d9e82867-d9af-4f3b-b2e8-d2b9210e1326",
   "metadata": {},
   "outputs": [],
   "source": [
    "with mlflow.start_run(run_name=\"build datasets\"):\n",
    "    # create Tensor datasets\n",
    "    train_data = TensorDataset(torch.from_numpy(X_train), torch.from_numpy(y_train))\n",
    "    valid_data = TensorDataset(torch.from_numpy(X_val), torch.from_numpy(y_val))\n",
    "    test_data = TensorDataset(torch.from_numpy(X_test), torch.from_numpy(y_test))\n",
    "    # dataloaders\n",
    "    batch_size = 64\n",
    "    # make sure to SHUFFLE your data\n",
    "    test_loader = DataLoader(test_data, shuffle=True, batch_size=batch_size)\n",
    "    \n",
    "    mlflow.log_param(\"operation\", 'buil datasets')\n",
    "    mlflow.log_param(\"requested version\", 'v4')\n",
    "    mlflow.log_param('version', 'v5')\n",
    "    mlflow.log_param('number of characters', '0.8, 0.1, 0.1')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1e014f7-fd84-4f3e-979d-a877b33bddfc",
   "metadata": {},
   "source": [
    "### build the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "79c113c0-1215-4452-b482-18a1895cd8ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO: 'build the model' does not exist. Creating a new experiment\n"
     ]
    }
   ],
   "source": [
    "mlflow.set_experiment('build the model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "04a19abf-ea04-445c-aeb4-3749e369a0c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SentimentRNN(nn.Module):\n",
    "    \"\"\"\n",
    "    The RNN model that will be used to perform Sentiment analysis.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, vocab_size, output_size, embedding_dim, hidden_dim, n_layers, drop_prob=0.5):\n",
    "        \"\"\"\n",
    "        Initialize the model by setting up the layers.\n",
    "        \"\"\"\n",
    "        super(SentimentRNN, self).__init__()\n",
    "\n",
    "        self.output_size = output_size\n",
    "        self.n_layers = n_layers\n",
    "        self.hidden_dim = hidden_dim\n",
    "        \n",
    "        # define all layers\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, n_layers, \n",
    "                            dropout=drop_prob, batch_first=True)\n",
    "        \n",
    "        self.dropout = nn.Dropout(0.3)\n",
    "        \n",
    "        self.fc = nn.Linear(hidden_dim, output_size)\n",
    "        self.sig = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x, hidden):\n",
    "        \"\"\"\n",
    "        Perform a forward pass of our model on some input and hidden state.\n",
    "        \"\"\"\n",
    "        batch_size = x.size(0)\n",
    "        \n",
    "        embeds = self.embedding(x)\n",
    "        lstm_out, hidden = self.lstm(embeds, hidden)\n",
    "        \n",
    "        lstm_out = lstm_out.contiguous().view(-1, self.hidden_dim)\n",
    "        \n",
    "        out = self.dropout(lstm_out)\n",
    "        out = self.fc(out)\n",
    "        \n",
    "        sig_out = self.sig(out)\n",
    "        \n",
    "        sig_out = sig_out.view(batch_size, -1)\n",
    "        sig_out = sig_out[:, -1]\n",
    "        # return last sigmoid output and hidden state\n",
    "        return sig_out, hidden\n",
    "    \n",
    "    \n",
    "    def init_hidden(self, batch_size):\n",
    "        ''' Initializes hidden state '''\n",
    "        # Create two new tensors with sizes n_layers x batch_size x hidden_dim,\n",
    "        # initialized to zero, for hidden state and cell state of LSTM\n",
    "        weight = next(self.parameters()).data\n",
    "        \n",
    "        if train_on_gpu:\n",
    "            hidden = (weight.new(self.n_layers, batch_size, self.hidden_dim).zero_().cuda(),\n",
    "                     weight.new(self.n_layers, batch_size, self.hidden_dim).zero_().cuda())\n",
    "        else:\n",
    "            hidden = (weight.new(self.n_layers, batch_size, self.hidden_dim).zero_(),\n",
    "                     weight.new(self.n_layers, batch_size, self.hidden_dim).zero_())\n",
    "        return hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "8226e7c3-7998-46fa-8468-ec09c8908130",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, n_epochs, optimizer, criterion, train_on_gpu, save_path, batch_size=64, print_every=10, clip=5):\n",
    "    print('Start Training on \"{}\" for {} epochs...'.format('GPU' if train_on_gpu else 'cpu', n_epochs))\n",
    "    train_loader = DataLoader(train_data, shuffle=True, batch_size=batch_size)\n",
    "    valid_loader = DataLoader(valid_data, shuffle=True, batch_size=batch_size)\n",
    "    # move model to GPU, if available\n",
    "    if(train_on_gpu):\n",
    "        net.cuda()\n",
    "\n",
    "   \n",
    "    val_loss_min = np.Inf\n",
    "    # train for some number of epochs\n",
    "    for e in range(n_epochs):\n",
    "        print('epoch:', e+1,'train...')\n",
    "        # initialize hidden state\n",
    "        h = model.init_hidden(batch_size)\n",
    "        \n",
    "        train_loss = 0\n",
    "        val_loss = 0\n",
    "        \n",
    "        train_total = 0\n",
    "        val_total = 0\n",
    "        # batch loop\n",
    "        net.train()\n",
    "        n_batches = len(train_loader.dataset) // batch_size\n",
    "        i = 1\n",
    "        for inputs, labels in tqdm(train_loader):\n",
    "            if i > n_batches:\n",
    "                break\n",
    "            i+=1\n",
    "            if(train_on_gpu):\n",
    "                inputs, labels = inputs.cuda(), labels.cuda()\n",
    "\n",
    "            # Creating new variables for the hidden state, otherwise\n",
    "            # we'd backprop through the entire training history\n",
    "            h = tuple([each.data for each in h])\n",
    "\n",
    "            # zero accumulated gradients\n",
    "            model.zero_grad()\n",
    "\n",
    "            # get the output from the model\n",
    "            output, h = model(inputs, h)\n",
    "\n",
    "            # calculate the loss and perform backprop\n",
    "            loss = criterion(output.squeeze(), labels.float())\n",
    "            \n",
    "            train_loss += loss.item() * len(labels)\n",
    "            train_total += len(labels)\n",
    "            \n",
    "            loss.backward()\n",
    "            # `clip_grad_norm` helps prevent the exploding gradient problem in RNNs / LSTMs.\n",
    "            nn.utils.clip_grad_norm_(net.parameters(), clip)\n",
    "            optimizer.step()\n",
    "\n",
    "        ############################## VALIDATION #################################\n",
    "        print('validation...')\n",
    "        val_h = net.init_hidden(batch_size)\n",
    "        net.eval()\n",
    "        n_batches = len(valid_loader.dataset) // batch_size\n",
    "        i = 1\n",
    "        for inputs, labels in tqdm(valid_loader):\n",
    "            if i > n_batches:\n",
    "                break\n",
    "            i += 1\n",
    "\n",
    "            # Creating new variables for the hidden state, otherwise\n",
    "            # we'd backprop through the entire training history\n",
    "            val_h = tuple([each.data for each in val_h])\n",
    "\n",
    "            if(train_on_gpu):\n",
    "                inputs, labels = inputs.cuda(), labels.cuda()\n",
    "\n",
    "            output, val_h = net(inputs, val_h)\n",
    "            val_loss = criterion(output.squeeze(), labels.float())\n",
    "\n",
    "            val_loss +=  val_loss.item() * len(labels)\n",
    "            val_total += len(labels)\n",
    "            \n",
    "        train_loss = train_loss / train_total\n",
    "        val_loss = val_loss / val_total\n",
    "        if e % print_every == 0:\n",
    "            print(\"Epoch: {}/{}...\".format(e+1, n_epochs),\n",
    "#                   \"Step: {}...\".format(counter),\n",
    "                  \"Train Loss: {:.6f}...\".format(train_loss),\n",
    "                  \"Val Loss: {:.6f}\".format(val_loss))\n",
    "            \n",
    "        if val_loss < val_loss_min:\n",
    "            print('Validation loss decreased from: {:.6f}, to: {:.6f}\\tSAVING MODEL... in Epoch: {}\\n'.format(val_loss_min, val_loss, e+1))\n",
    "            \n",
    "            # save the model\n",
    "            torch.save(model.state_dict(), save_path)\n",
    "            \n",
    "            # update minimum val loss\n",
    "            val_loss_min = val_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "75a9f231-5ffc-4ac8-860b-ee3e6306ea39",
   "metadata": {},
   "outputs": [],
   "source": [
    "with mlflow.start_run(run_name=\"model structure\"):\n",
    "\n",
    "    vocab_size = len(vocab_to_int) + 1 # +1 for 0 padding\n",
    "    output_size = 1 # pos, or neg\n",
    "    embedding_dim = 100\n",
    "    hidden_dim = 128\n",
    "    n_layers = 2\n",
    "    net = SentimentRNN(vocab_size, output_size, embedding_dim, hidden_dim, n_layers)\n",
    "    \n",
    "    mlflow.log_param(\"operation\", 'instantiate the model')\n",
    "    mlflow.log_param(\"embedding_dim\", embedding_dim)\n",
    "    mlflow.log_param('hidden_dm', hidden_dim)\n",
    "    mlflow.log_param('lstm_layers', n_layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e54e389-44f0-45f4-90f9-3c979da3ad51",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
